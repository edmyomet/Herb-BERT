{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/shruthi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/shruthi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer, WordPunctTokenizer, TreebankWordTokenizer,word_tokenize\n",
    "from transformers import BertTokenizer, AlbertTokenizer, GPT2Tokenizer\n",
    "from transformers import WordpieceTokenizer,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lowercaser:\n",
    "    dataset:pd.core.frame.DataFrame\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dataset = kwargs['dataset']\n",
    "    def convert_to_lowercase(self):\n",
    "        for col in self.dataset.columns:\n",
    "            self.dataset[col] = self.dataset[col].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "    \n",
    "    def get_dataset(self)->pd.core.frame.DataFrame:\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatiser:\n",
    "    dataset:pd.core.frame.DataFrame\n",
    "    lemmatizer=None\n",
    "    def __init__(self,**kwargs):\n",
    "        self.dataset = kwargs['dataset']\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def __lemmatise(self,**kwargs)->None:\n",
    "        field:str = kwargs['column']\n",
    "        self.dataset.loc[:,field] = self.dataset.apply(lambda row: self.lemmatizer.lemmatize(row[field]),axis=1)\n",
    "        print(self.dataset[field])\n",
    "    \n",
    "    def lemmatise(self)->None:\n",
    "        for fields in self.dataset.columns:\n",
    "            self.__lemmatise(column=fields)\n",
    "    \n",
    "    def get_dataset(self)->pd.core.frame.DataFrame:\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    dataset:pd.core.frame.DataFrame=None\n",
    "    tokeniser_choice:str = \"\"\n",
    "    tokeniser=None\n",
    "    word_tokeniser=None\n",
    "    tokenised_dataset:pd.core.frame.DataFrame=None\n",
    "    def __init__(self,**kwargs):\n",
    "        self.dataset = kwargs['dataset']\n",
    "        self.tokeniser_choice = kwargs['token_choice']\n",
    "        self.tokenised_dataset = pd.DataFrame(columns=self.dataset.columns)\n",
    "    \n",
    "    def __select_tokeniser(self)->None:\n",
    "        match(self.tokeniser_choice):\n",
    "            case 'BertTokeniser':\n",
    "                self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased',tokenization_strategy='word',model_max_length=512)\n",
    "            case _:\n",
    "                self.word_tokeniser = word_tokenize()\n",
    "                self.tokeniser = MWETokenizer()\n",
    "                \n",
    "            # case 'AutoTokeniser':\n",
    "            #     self.tokeniser = AutoTokenizer.from_pretrained(\"obi/deid_bert_i2b2\", tokenizer_args={\"do_basic_tokenize\": False})\n",
    "            # case 'gpt2':\n",
    "            #     self.tokeniser = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    def __tokenise(self,**kwargs)->list[list[str]]:\n",
    "        sentences = kwargs['data_sentence']\n",
    "        self.__select_tokeniser()\n",
    "        tokens:list[str] = []\n",
    "        tmp_tokens = self.tokeniser.encode_plus(sentences,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    max_length=20,\n",
    "                                                    padding='max_length',\n",
    "                                                    truncation=True,\n",
    "                                                    return_tensors='pt')\n",
    "        tokens.append(tmp_tokens['input_ids'].flatten().tolist()[0])\n",
    "        return tokens\n",
    "    \n",
    "    def tokenise(self)->None:\n",
    "        # for col in self.dataset.columns:\n",
    "        #     self.dataset.loc[0:10:,col] = self.dataset.apply(lambda x: self.__tokenise(data_sentence=x[col]),axis=1)\n",
    "        self.dataset.loc[:,'Action'] = self.dataset.apply(lambda x:self.__tokenise(data_sentence=x['Action']),axis=1)\n",
    "        print(self.dataset['Action'])\n",
    "    def get_dataset(self)->pd.core.frame.DataFrame:\n",
    "        return self.dataset\n",
    "    \n",
    "    def save_dataset(self)->pd.core.frame.DataFrame:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Global Model Configurations for Interpolation\n",
    "'''\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_tokeniser = BertTokenizer.from_pretrained('bert-base-uncased',tokenization_strategy='word')\n",
    "fill_mask = pipeline('fill-mask',model=bert_model,tokenizer=bert_tokeniser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(**kwargs)->dict:\n",
    "    file_path:str = kwargs['file_path']\n",
    "    data:dict[str,list[dict[str,str]]]={}\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json_obj(**kwargs)->pd.core.frame.DataFrame:\n",
    "    data = kwargs['json_unflattened']\n",
    "    sub_data:dict[str,dict[str,str]] = data['root'][0]\n",
    "    column_list:set[str] = set()\n",
    "    for name, sub_dict in sub_data.items():\n",
    "        for key in sub_dict.keys():\n",
    "            if key not in column_list:\n",
    "                column_list.add(key)\n",
    "    print(column_list)\n",
    "    dataset = pd.DataFrame.from_dict(data=sub_data,orient='index',columns=list(column_list))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(**kwargs)->pd.core.frame.DataFrame:\n",
    "    lc = Lowercaser(dataset=kwargs['dataset'])\n",
    "    lc.convert_to_lowercase()\n",
    "    return lc.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_with_gpt(**kwargs)->str:\n",
    "    text_generator = pipeline('text-generation',model='gpt2')\n",
    "    prompt:str = kwargs['context_before'] + \"[MISSING]\" + kwargs['context_after']\n",
    "    text:str = kwargs['text']\n",
    "    if pd.isna(text):\n",
    "        generated_text = text_generator(prompt, max_length=50)[0]['generated_text']\n",
    "        return generated_text.replace(prompt,\"\").strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_with_bert(**kwargs)->str:\n",
    "    text:str = kwargs['text']\n",
    "    if pd.isna(text):\n",
    "        sentence = kwargs['context_before'] + \"[MASK]\" + kwargs['context_after']\n",
    "        predictions = fill_mask(sentence)\n",
    "        return predictions[0]['sequence']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_values(**kwargs)->pd.core.frame.DataFrame:\n",
    "    dataset = kwargs['dataset']\n",
    "    column = kwargs['col']\n",
    "    col_before = kwargs['col_before']\n",
    "    dataset.loc[:,column] = dataset.apply(lambda row: interpolation_with_bert(text=row[column],context_before=row[col_before],context_after=\"\"),axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatisation(**kwargs)->None:\n",
    "    dataset = kwargs['dataset']\n",
    "    tokeniser = Tokeniser(dataset=dataset, token_choice='BertTokeniser')\n",
    "    tokeniser.tokenise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(**kwargs)->None:\n",
    "    dataset:pd.core.frame.DataFrame = kwargs['dataset']\n",
    "    fields:list[str] = list(dataset.columns)\n",
    "    #performing lowercasing operation \n",
    "    dataset = lowercasing(dataset=dataset)\n",
    "    \n",
    "    #interpolating missing values by taking advantage of the masking property of a pre-trained BERT model\n",
    "    for col_index in range(len(fields[0:-2])):\n",
    "        dataset = interpolate_missing_values(dataset=dataset, col=fields[col_index],col_before=fields[col_index-1])\n",
    "    print(dataset.isna())\n",
    "    \n",
    "    #tokenising and lemmatising the dataset,forming an entirely new dataset of padded tokens\n",
    "    lemmatisation(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    obj = load_json_data(file_path=rf'../data/json_data1.json')\n",
    "    dataset = flatten_json_obj(json_unflattened=obj)\n",
    "    preprocess(dataset=dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
